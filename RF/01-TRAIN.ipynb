{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a RF to predict offsets of absolute FEP predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jscheen/miniconda3/envs/freesolv/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/jscheen/miniconda3/envs/freesolv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# General:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "from matplotlib.ticker import AutoMinorLocator, FormatStrFormatter\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import shutil\n",
    "import logging\n",
    "import pickle\n",
    "import statistics\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# SVM:\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# SciKit-Optimise:\n",
    "from skopt import gp_minimize, dump\n",
    "from skopt.space import Categorical, Integer, Real\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables:\n",
    "model_type = 'RF'\n",
    "\n",
    "label_col = 'dGoffset (kcal/mol)'\n",
    "null_label_col = 'Experimental dGhydr (kcal/mol)'\n",
    "\n",
    "# Set data processing configurations:\n",
    "n_calls = 65  # Number of Bayesian optimisation loops for hyper-parameter optimisation, 40 is best for convergence, > 60 scales to very expensive\n",
    "best_mae = np.inf  # Point to consider top-performing model from (MAE/MAD); 1.0 = no improvement on test-set variance\n",
    "\n",
    "# KFold parameters:\n",
    "n_splits = 5  # Number of K-fold splits\n",
    "random_state = 2  # Random number seed\n",
    "\n",
    "# RDKit:\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import SDMolSupplier\n",
    "from rdkit import DataStructs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = [7, 7]\n",
    "fontsize = 18\n",
    "markersize = 20\n",
    "linewidth = 2\n",
    "dGhydr_xylim = [-25.0, 5.0]\n",
    "dGoffset_xylim = [-4.0, 2.0]\n",
    "dpi = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path variables:\n",
    "datasets_dr = '../datasets/DATASETS/'\n",
    "SDF_dr = '../datasets/backend/freesolv/'\n",
    "freesolv_loc = '../datasets/backend/freesolv_database.txt'\n",
    "\n",
    "# clean slate output_dr\n",
    "output_dr = 'output/'\n",
    "if not os.path.exists(output_dr):\n",
    "    os.mkdir(output_dr)\n",
    "\n",
    "# clean slate figures_dr\n",
    "figures_dr = 'figures/'\n",
    "if not os.path.exists(figures_dr):\n",
    "    os.mkdir(figures_dr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = [\"APFP\", \"ECFP6\", \"TOPOL\", \"MolProps\", \"MolPropsAPFP\", \"MolPropsECFP6\", \"MolPropsTOPOL\", \"X-NOISE\"]\n",
    "training_sets = []\n",
    "for feature_set in feature_sets:\n",
    "    # make output directory:\n",
    "    if not os.path.exists(\"output/\"+feature_set):\n",
    "        os.mkdir(\"output/\"+feature_set)\n",
    "    \n",
    "    # load in this feature set:\n",
    "    train_path = datasets_dr + 'train_'+feature_set+'.csv'\n",
    "    train_df = pd.read_csv(train_path, index_col='ID').iloc[:,:-1]\n",
    "    # assign a name attribute to recover feature type (called by df.name)\n",
    "    train_df.name = feature_set\n",
    "    training_sets.append(train_df)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list containing all training sets. For each training set, we want to train using cross-validation because we have little data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, n_splits, random_state, label_name, feature_type):\n",
    "    \"\"\"KFold implementation for pandas DataFrame.\n",
    "    (https://stackoverflow.com/questions/45115964/separate-pandas-dataframe-using-sklearns-kfold)\"\"\"\n",
    "    \n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    kfolds = []\n",
    "    global offset_col_name\n",
    "\n",
    "    for train, validate in kf.split(dataset):\n",
    "        training = dataset.iloc[train]\n",
    "        train_labels = training[label_name]\n",
    "        train_set = training.drop(label_name, axis=1)\n",
    "\n",
    "        validating = dataset.iloc[validate]\n",
    "        validate_labels = validating[label_name]\n",
    "        validate_set = validating.drop(label_name, axis=1)\n",
    "\n",
    "        kfolds.append(\n",
    "            [[train_set, validate_set],\n",
    "             [train_labels, validate_labels]]\n",
    "        )\n",
    "\n",
    "\n",
    "    return kfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Call function\n",
    "CV_training_sets = []\n",
    "for train_df in training_sets:\n",
    "    kfolds = [train_df.name, split_dataset(\n",
    "                        train_df, \n",
    "                        n_splits, \n",
    "                        random_state, \n",
    "                        label_col, \n",
    "                        feature_type=train_df.name\n",
    "                            )]\n",
    "    CV_training_sets.append(kfolds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a nested list of the shape [feature_type [[cross-validation folds]], ...]. Next, loop over this list and train per feature type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyper-parameter ranges and append to list.\n",
    "dim_num_estimators = Integer(1, 1000, name='n_estimators')\n",
    "dim_max_depth = Integer(1, 5, name='max_depth')\n",
    "dim_min_samples_split = Integer(2, 10, name='min_samples_split')\n",
    "dim_bootstrap = Categorical(categories=[True, False], name=\"bootstrap\")\n",
    "\n",
    "# gp_minimize dimensions\n",
    "dimensions = [dim_num_estimators, dim_max_depth, dim_min_samples_split,dim_bootstrap]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_estimators, max_depth, min_samples_split, bootstrap):\n",
    "    \"\"\"Returns a RF class instance.\"\"\"\n",
    "    \n",
    "    return RandomForestRegressor(\n",
    "        n_estimators = n_estimators, \n",
    "        max_depth = max_depth, \n",
    "        min_samples_split = min_samples_split,\n",
    "        bootstrap = bootstrap,\n",
    "        verbose=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(fold, fold_num, feature_type, writer, replicate_tag, *args):\n",
    "    \"\"\"\n",
    "    1. Unpack data.\n",
    "    2. Define fitness function for guassian process optmisation.\n",
    "    3. Decorate fitness with pre-defined hyper-parameter ranges.\n",
    "    4. Return and pickle final gp_minimize object.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Retrieve data sets and convert to numpy array\n",
    "    train_X = fold[0][0].values\n",
    "    validate_X = fold[0][1].values\n",
    "    train_y = fold[1][0].values\n",
    "    validate_y = fold[1][1].values\n",
    "    \n",
    "    # Specify whether model is null.\n",
    "    null = ''\n",
    "    for arg in args:\n",
    "        null = arg + '_'\n",
    "    \n",
    "    # Define function for gaussian process optimisation\n",
    "    @use_named_args(dimensions=dimensions)\n",
    "    def fitness(n_estimators, max_depth, min_samples_split, bootstrap):\n",
    "        \"\"\"Function for gaussian process optmisation.\"\"\"\n",
    "\n",
    "        # Create SVR model\n",
    "        model = create_model(n_estimators, max_depth, min_samples_split, bootstrap)\n",
    "\n",
    "        # Train model on training data\n",
    "        model.fit(train_X, train_y)\n",
    "\n",
    "        # Validate model\n",
    "        predicted_y = model.predict(validate_X)\n",
    "        mae = mean_absolute_error(validate_y, predicted_y)\n",
    "\n",
    "        # Update statistics\n",
    "        writer.writerow([fold_num, mae, [n_estimators, max_depth, min_samples_split, bootstrap]])\n",
    "\n",
    "        # Check if model improves\n",
    "        global best_mae\n",
    "        if mae < best_mae:\n",
    "            # Update new model accuracy.\n",
    "            best_mae = mae\n",
    "            # Overwrite model if mae improves\n",
    "            pkl_file = output_dr + feature_type+'/fold_' + str(fold_num) + '_' + model_type + replicate_tag+'_model.pickle'\n",
    "            with open(pkl_file, 'wb') as file: pickle.dump(model, file)\n",
    "        \n",
    "        return mae\n",
    "    \n",
    "    # Starting parameters\n",
    "    default_parameters = [1, 1, 2, True]\n",
    "    # Optimise hyper-parameters\n",
    "    search_result = gp_minimize(func=fitness,\n",
    "                                dimensions=dimensions,\n",
    "                                acq_func='EI',  # Expected Improvement.\n",
    "                                n_calls=n_calls,\n",
    "                                x0=default_parameters,\n",
    "                                verbose=False,\n",
    "                                #callback=[tqdm_skopt(total=n_calls, desc='Fold {}'.format(fold_num))]\n",
    "                               )\n",
    "\n",
    "    # Save skopt object; finish up:\n",
    "\n",
    "    \n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform training per fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: APFP repl1\n",
      "Training on: ECFP6 repl1\n",
      "Training on: TOPOL repl1\n",
      "Training on: MolProps repl1\n",
      "Training on: MolPropsAPFP repl1\n",
      "Training on: MolPropsECFP6 repl1\n",
      "Training on: MolPropsTOPOL repl1\n",
      "Training on: X-NOISE repl1\n",
      "Training on: APFP repl2\n",
      "Training on: ECFP6 repl2\n",
      "Training on: TOPOL repl2\n",
      "Training on: MolProps repl2\n",
      "Training on: MolPropsAPFP repl2\n",
      "Training on: MolPropsECFP6 repl2\n",
      "Training on: MolPropsTOPOL repl2\n",
      "Training on: X-NOISE repl2\n",
      "Training on: APFP repl3\n",
      "Training on: ECFP6 repl3\n",
      "Training on: TOPOL repl3\n",
      "Training on: MolProps repl3\n",
      "Training on: MolPropsAPFP repl3\n",
      "Training on: MolPropsECFP6 repl3\n",
      "Training on: MolPropsTOPOL repl3\n",
      "Training on: X-NOISE repl3\n",
      "Training on: APFP repl4\n",
      "Training on: ECFP6 repl4\n",
      "Training on: TOPOL repl4\n",
      "Training on: MolProps repl4\n",
      "Training on: MolPropsAPFP repl4\n",
      "Training on: MolPropsECFP6 repl4\n",
      "Training on: MolPropsTOPOL repl4\n",
      "Training on: X-NOISE repl4\n",
      "Training on: APFP repl5\n",
      "Training on: ECFP6 repl5\n",
      "Training on: TOPOL repl5\n",
      "Training on: MolProps repl5\n",
      "Training on: MolPropsAPFP repl5\n",
      "Training on: MolPropsECFP6 repl5\n",
      "Training on: MolPropsTOPOL repl5\n",
      "Training on: X-NOISE repl5\n",
      "Training on: APFP repl6\n",
      "Training on: ECFP6 repl6\n",
      "Training on: TOPOL repl6\n",
      "Training on: MolProps repl6\n",
      "Training on: MolPropsAPFP repl6\n",
      "Training on: MolPropsECFP6 repl6\n",
      "Training on: MolPropsTOPOL repl6\n",
      "Training on: X-NOISE repl6\n",
      "Training on: APFP repl7\n",
      "Training on: ECFP6 repl7\n"
     ]
    }
   ],
   "source": [
    "# Run 10 replicates:\n",
    "for replicate in range(1,11):\n",
    "    replicate_tag = \"repl\"+str(replicate)\n",
    "\n",
    "    # Call train_model and save gp_minimize results.\n",
    "    for kfolds in CV_training_sets:\n",
    "        name = kfolds[0]\n",
    "        kfolds = kfolds[1]\n",
    "        print(\"Training on:\", name, replicate_tag)\n",
    "\n",
    "        # Init. training statistics logger.\n",
    "\n",
    "        with open('output/'+name+'/mae_per_call'+replicate_tag+'.csv', 'w') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Fold', 'MAE (kcal/mol)', 'Hyperparameters'])\n",
    "\n",
    "            for fold_num, fold in enumerate(kfolds):\n",
    "                best_mae = np.inf\n",
    "                gp_results = train_model(fold, \n",
    "                                          fold_num + 1, \n",
    "                                          feature_type=name, \n",
    "                                          writer=writer,\n",
    "                                          replicate_tag=replicate_tag)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_convergence(MAE_output_dfs):\n",
    "\n",
    "    # x values\n",
    "    x = np.linspace(1, n_calls-1, n_calls-1)\n",
    "\n",
    "    for MAE_outputs in MAE_output_dfs:\n",
    "        MAEs_all_reps = []\n",
    "        for MAE_output in MAE_outputs:\n",
    "\n",
    "            result_maes = MAE_output[2]\n",
    "            name = MAE_output[1]\n",
    "            # y values\n",
    "            mae = [result_maes.loc[result_maes.iloc[:, 0] == fold, 'MAE (kcal/mol)'].cummin()\n",
    "                   for fold in range(1, n_splits + 1)]\n",
    "            cumm_mae = list(zip(*mae))\n",
    "            mean_MAEs_this_rep = [statistics.mean(call) for call in cumm_mae]\n",
    "            MAEs_all_reps.append(np.array(mean_MAEs_this_rep[:49]))\n",
    "        \n",
    "        y = np.mean(MAEs_all_reps, axis=0)\n",
    "        std = np.std(MAEs_all_reps, axis=0)\n",
    "        \n",
    "\n",
    "        # standard devation bounds\n",
    "        y1 = [i - sd for i, sd in zip(y, std)]\n",
    "        y2 = [i + sd for i, sd in zip(y, std)]\n",
    "        \n",
    "        plt.plot(x, y,\n",
    "                linewidth=linewidth, label=name\n",
    "                #label='Average MAE over {} folds'.format(n_splits)\n",
    "               )\n",
    "        # plot standard deviation fill bounds\n",
    "        plt.fill_between(x, y1, y2,\n",
    "                        alpha=0.2,\n",
    "                        #label='Standard deviation'\n",
    "                       )\n",
    "        plt.legend(bbox_to_anchor=(1.1, 1.05))\n",
    "    plt.xlabel('Hyperparameter optimisation progression / n calls')\n",
    "    plt.ylabel('Mean absolute error / kcal mol$^{-1}$')\n",
    "    plt.show()\n",
    "        \n",
    "#     # Setup figure.\n",
    "#     fig, axes = plt.subplots(nrows=1, ncols=len(feature_sets), sharey=True)\n",
    "#     fig.set_size_inches(18.5, 7)\n",
    "    \n",
    "#     for ax, MAE_outputs in zip(axes, MAE_output_dfs):\n",
    "#         MAEs_all_reps = []\n",
    "#         for MAE_output in MAE_outputs:\n",
    "\n",
    "#             result_maes = MAE_output[2]\n",
    "#             name = MAE_output[1]\n",
    "#             # y values\n",
    "#             mae = [result_maes.loc[result_maes.iloc[:, 0] == fold, 'MAE (kcal/mol)'].cummin()\n",
    "#                    for fold in range(1, n_splits + 1)]\n",
    "#             cumm_mae = list(zip(*mae))\n",
    "#             mean_MAEs_this_rep = [statistics.mean(call) for call in cumm_mae]\n",
    "#             MAEs_all_reps.append(mean_MAEs_this_rep)\n",
    "        \n",
    "#         y = np.mean(MAEs_all_reps, axis=0)\n",
    "#         std = np.std(MAEs_all_reps, axis=0)\n",
    "        \n",
    "\n",
    "#         # standard devation bounds\n",
    "#         y1 = [i - sd for i, sd in zip(y, std)]\n",
    "#         y2 = [i + sd for i, sd in zip(y, std)]\n",
    "        \n",
    "#         ax.plot(x, y,\n",
    "#                 linewidth=linewidth,\n",
    "#                 #label='Average MAE over {} folds'.format(n_splits)\n",
    "#                )\n",
    "#         # plot standard deviation fill bounds\n",
    "#         ax.fill_between(x, y1, y2,\n",
    "#                         alpha=0.2,\n",
    "#                         #label='Standard deviation'\n",
    "#                        )\n",
    "#         # more formatting:\n",
    "#         ax.set_title(name)\n",
    "   \n",
    "\n",
    "#     axes[3].set_xlabel('Hyperparameter optimisation progression / n calls', fontsize=fontsize)\n",
    "#     axes[0].set_ylabel('Mean absolute error / kcal mol$^{-1}$', fontsize=fontsize)\n",
    "\n",
    "#     #fig.savefig(figures_dr + 'convergence_plot.png', dpi=dpi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make a list with all our MAE outputs:\n",
    "MAE_output_dfs = []\n",
    "replicate_tags = [ \"repl\"+str(value) for value in np.arange(3,11) ]\n",
    "maes_per_featureset = []\n",
    "for feature_set in feature_sets:\n",
    "    maes_all_replicates = []\n",
    "    for replicate in replicate_tags:\n",
    "        mae_logger = output_dr + feature_set + '/mae_per_call'+replicate+'.csv'\n",
    "        mae_df = pd.read_csv(mae_logger, usecols=[0,1]).head(n_calls*n_splits)  # Load previously defined training statistics logger.\n",
    "        maes_all_replicates.append([replicate, feature_set, mae_df])\n",
    "    maes_per_featureset.append(maes_all_replicates)\n",
    "        \n",
    "\n",
    "# Plot convergence plots.\n",
    "\n",
    "mae_convergence(maes_per_featureset)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All generated models and logs are saved in ./output/\n",
    "Note that because of hyperparameter optimision per fold, each fold model will have a different\n",
    "hyperparameter configuration. The specific configurations can be retrieved from the outputs above.\n",
    "\n",
    "\n",
    "The convergence plot is saved ./figures/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
