{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a RF to predict offsets of absolute FEP predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jscheen/miniconda3/envs/freesolv/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/jscheen/miniconda3/envs/freesolv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# General:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "from matplotlib.ticker import AutoMinorLocator, FormatStrFormatter\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import shutil\n",
    "import logging\n",
    "import pickle\n",
    "import statistics\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# SVM:\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# SciKit-Optimise:\n",
    "from skopt import gp_minimize, dump\n",
    "from skopt.space import Categorical, Integer, Real\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables:\n",
    "model_type = 'RF'\n",
    "\n",
    "label_col = 'dGoffset (kcal/mol)'\n",
    "null_label_col = 'Experimental dGhydr (kcal/mol)'\n",
    "\n",
    "# Set data processing configurations:\n",
    "n_calls = 50  # Number of Bayesian optimisation loops for hyper-parameter optimisation, 40 is best for convergence, > 60 scales to very expensive\n",
    "best_mae = np.inf  # Point to consider top-performing model from (MAE/MAD); 1.0 = no improvement on test-set variance\n",
    "\n",
    "# KFold parameters:\n",
    "n_splits = 5  # Number of K-fold splits\n",
    "random_state = 2  # Random number seed\n",
    "\n",
    "# RDKit:\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import SDMolSupplier\n",
    "from rdkit import DataStructs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = [7, 7]\n",
    "fontsize = 18\n",
    "markersize = 20\n",
    "linewidth = 2\n",
    "dGhydr_xylim = [-25.0, 5.0]\n",
    "dGoffset_xylim = [-4.0, 2.0]\n",
    "dpi = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path variables:\n",
    "datasets_dr = '../datasets/DATASETS/'\n",
    "SDF_dr = '../datasets/backend/freesolv/'\n",
    "freesolv_loc = '../datasets/backend/freesolv_database.txt'\n",
    "\n",
    "# clean slate output_dr\n",
    "output_dr = 'output/'\n",
    "if not os.path.exists(output_dr):\n",
    "    os.mkdir(output_dr)\n",
    "\n",
    "# clean slate figures_dr\n",
    "figures_dr = 'figures/'\n",
    "if not os.path.exists(figures_dr):\n",
    "    os.mkdir(figures_dr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = [\"APFP\", \"ECFP6\", \"TOPOL\", \"MolProps\", \"MolPropsAPFP\", \"MolPropsECFP6\", \"MolPropsTOPOL\", \"X-NOISE\"]\n",
    "training_sets = []\n",
    "for feature_set in feature_sets:\n",
    "    # make output directory:\n",
    "    if not os.path.exists(\"output/\"+feature_set):\n",
    "        os.mkdir(\"output/\"+feature_set)\n",
    "    \n",
    "    # load in this feature set:\n",
    "    train_path = datasets_dr + 'train_'+feature_set+'.csv'\n",
    "    train_df = pd.read_csv(train_path, index_col='ID').iloc[:,:-1]\n",
    "    # assign a name attribute to recover feature type (called by df.name)\n",
    "    train_df.name = feature_set\n",
    "    training_sets.append(train_df)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list containing all training sets. For each training set, we want to train using cross-validation because we have little data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, n_splits, random_state, label_name, feature_type):\n",
    "    \"\"\"KFold implementation for pandas DataFrame.\n",
    "    (https://stackoverflow.com/questions/45115964/separate-pandas-dataframe-using-sklearns-kfold)\"\"\"\n",
    "    \n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    kfolds = []\n",
    "    global offset_col_name\n",
    "\n",
    "    for train, validate in kf.split(dataset):\n",
    "        training = dataset.iloc[train]\n",
    "        train_labels = training[label_name]\n",
    "        train_set = training.drop(label_name, axis=1)\n",
    "\n",
    "        validating = dataset.iloc[validate]\n",
    "        validate_labels = validating[label_name]\n",
    "        validate_set = validating.drop(label_name, axis=1)\n",
    "\n",
    "        kfolds.append(\n",
    "            [[train_set, validate_set],\n",
    "             [train_labels, validate_labels]]\n",
    "        )\n",
    "\n",
    "\n",
    "    return kfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Call function\n",
    "CV_training_sets = []\n",
    "for train_df in training_sets:\n",
    "    kfolds = [train_df.name, split_dataset(\n",
    "                        train_df, \n",
    "                        n_splits, \n",
    "                        random_state, \n",
    "                        label_col, \n",
    "                        feature_type=train_df.name\n",
    "                            )]\n",
    "    CV_training_sets.append(kfolds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a nested list of the shape [feature_type [[cross-validation folds]], ...]. Next, loop over this list and train per feature type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyper-parameter ranges and append to list.\n",
    "dim_num_estimators = Integer(1, 1000, name='n_estimators')\n",
    "dim_max_depth = Integer(1, 5, name='max_depth')\n",
    "dim_min_samples_split = Integer(2, 10, name='min_samples_split')\n",
    "dim_bootstrap = Categorical(categories=[True, False], name=\"bootstrap\")\n",
    "\n",
    "# gp_minimize dimensions\n",
    "dimensions = [dim_num_estimators, dim_max_depth, dim_min_samples_split,dim_bootstrap]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_estimators, max_depth, min_samples_split, bootstrap):\n",
    "    \"\"\"Returns a RF class instance.\"\"\"\n",
    "    \n",
    "    return RandomForestRegressor(\n",
    "        n_estimators = n_estimators, \n",
    "        max_depth = max_depth, \n",
    "        min_samples_split = min_samples_split,\n",
    "        bootstrap = bootstrap,\n",
    "        verbose=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(fold, fold_num, feature_type, writer, replicate_tag, *args):\n",
    "    \"\"\"\n",
    "    1. Unpack data.\n",
    "    2. Define fitness function for guassian process optmisation.\n",
    "    3. Decorate fitness with pre-defined hyper-parameter ranges.\n",
    "    4. Return and pickle final gp_minimize object.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Retrieve data sets and convert to numpy array\n",
    "    train_X = fold[0][0].values\n",
    "    validate_X = fold[0][1].values\n",
    "    train_y = fold[1][0].values\n",
    "    validate_y = fold[1][1].values\n",
    "    \n",
    "    # Specify whether model is null.\n",
    "    null = ''\n",
    "    for arg in args:\n",
    "        null = arg + '_'\n",
    "    \n",
    "    # Define function for gaussian process optimisation\n",
    "    @use_named_args(dimensions=dimensions)\n",
    "    def fitness(n_estimators, max_depth, min_samples_split, bootstrap):\n",
    "        \"\"\"Function for gaussian process optmisation.\"\"\"\n",
    "\n",
    "        # Create SVR model\n",
    "        model = create_model(n_estimators, max_depth, min_samples_split, bootstrap)\n",
    "\n",
    "        # Train model on training data\n",
    "        model.fit(train_X, train_y)\n",
    "\n",
    "        # Validate model\n",
    "        predicted_y = model.predict(validate_X)\n",
    "        mae = mean_absolute_error(validate_y, predicted_y)\n",
    "\n",
    "        # Update statistics\n",
    "        writer.writerow([fold_num, mae, [dim_num_estimators, dim_max_depth, dim_min_samples_split,dim_bootstrap]])\n",
    "\n",
    "        # Check if model improves\n",
    "        global best_mae\n",
    "        if mae < best_mae:\n",
    "            # Update new model accuracy.\n",
    "            best_mae = mae\n",
    "            # Overwrite model if mae improves\n",
    "            pkl_file = output_dr + feature_type+'/fold_' + str(fold_num) + '_' + model_type + replicate_tag+'_model.pickle'\n",
    "            with open(pkl_file, 'wb') as file: pickle.dump(model, file)\n",
    "        \n",
    "        return mae\n",
    "    \n",
    "    # Starting parameters\n",
    "    default_parameters = [1, 1, 2, True]\n",
    "    # Optimise hyper-parameters\n",
    "    search_result = gp_minimize(func=fitness,\n",
    "                                dimensions=dimensions,\n",
    "                                acq_func='EI',  # Expected Improvement.\n",
    "                                n_calls=n_calls,\n",
    "                                x0=default_parameters,\n",
    "                                verbose=False,\n",
    "                                #callback=[tqdm_skopt(total=n_calls, desc='Fold {}'.format(fold_num))]\n",
    "                               )\n",
    "\n",
    "    # Save skopt object; finish up:\n",
    "\n",
    "    \n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform training per fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: APFP repl1\n",
      "Training on: ECFP6 repl1\n",
      "Training on: TOPOL repl1\n",
      "Training on: MolProps repl1\n",
      "Training on: MolPropsAPFP repl1\n",
      "Training on: MolPropsECFP6 repl1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ee95937027da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                           \u001b[0mfeature_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                           \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                                           replicate_tag=replicate_tag)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-948c79ead912>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(fold, fold_num, feature_type, writer, replicate_tag, *args)\u001b[0m\n\u001b[1;32m     56\u001b[0m                                 \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                                 \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                                 \u001b[0;31m#callback=[tqdm_skopt(total=n_calls, desc='Fold {}'.format(fold_num))]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                                )\n",
      "\u001b[0;32m~/miniconda3/envs/freesolv/lib/python3.7/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         callback=callback, n_jobs=n_jobs)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/freesolv/lib/python3.7/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/freesolv/lib/python3.7/site-packages/skopt/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;31m# Call the wrapped objective function with the named arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             \u001b[0mobjective_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-948c79ead912>\u001b[0m in \u001b[0;36mfitness\u001b[0;34m(n_estimators, max_depth, min_samples_split, bootstrap)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Train model on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Validate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/freesolv/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[0;32m--> 383\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/freesolv/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/freesolv/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/freesolv/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/freesolv/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/freesolv/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/freesolv/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/freesolv/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/freesolv/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/freesolv/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/freesolv/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run 10 replicates:\n",
    "for replicate in range(1,11):\n",
    "    replicate_tag = \"repl\"+str(replicate)\n",
    "\n",
    "    # Call train_model and save gp_minimize results.\n",
    "    for kfolds in CV_training_sets:\n",
    "        name = kfolds[0]\n",
    "        kfolds = kfolds[1]\n",
    "        print(\"Training on:\", name, replicate_tag)\n",
    "\n",
    "        # Init. training statistics logger.\n",
    "\n",
    "        with open('output/'+name+'/mae_per_call'+replicate_tag+'.csv', 'w') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Fold', 'MAE (kcal/mol)', 'Hyperparameters'])\n",
    "\n",
    "            for fold_num, fold in enumerate(kfolds):\n",
    "                best_mae = np.inf\n",
    "                gp_results = train_model(fold, \n",
    "                                          fold_num + 1, \n",
    "                                          feature_type=name, \n",
    "                                          writer=writer,\n",
    "                                          replicate_tag=replicate_tag)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_convergence(MAE_output_dfs):\n",
    "\n",
    "    # x values\n",
    "    x = np.linspace(1, n_calls, n_calls)\n",
    "\n",
    "    for MAE_outputs in MAE_output_dfs:\n",
    "        MAEs_all_reps = []\n",
    "        for MAE_output in MAE_outputs:\n",
    "\n",
    "            result_maes = MAE_output[2]\n",
    "            name = MAE_output[1]\n",
    "            # y values\n",
    "            mae = [result_maes.loc[result_maes.iloc[:, 0] == fold, 'MAE (kcal/mol)'].cummin()\n",
    "                   for fold in range(1, n_splits + 1)]\n",
    "            cumm_mae = list(zip(*mae))\n",
    "            mean_MAEs_this_rep = [statistics.mean(call) for call in cumm_mae]\n",
    "            MAEs_all_reps.append(mean_MAEs_this_rep)\n",
    "        \n",
    "        y = np.mean(MAEs_all_reps, axis=0)\n",
    "        std = np.std(MAEs_all_reps, axis=0)\n",
    "        \n",
    "\n",
    "        # standard devation bounds\n",
    "        y1 = [i - sd for i, sd in zip(y, std)]\n",
    "        y2 = [i + sd for i, sd in zip(y, std)]\n",
    "        \n",
    "        plt.plot(x, y,\n",
    "                linewidth=linewidth, label=name\n",
    "                #label='Average MAE over {} folds'.format(n_splits)\n",
    "               )\n",
    "        # plot standard deviation fill bounds\n",
    "        plt.fill_between(x, y1, y2,\n",
    "                        alpha=0.2,\n",
    "                        #label='Standard deviation'\n",
    "                       )\n",
    "        plt.legend(bbox_to_anchor=(1.1, 1.05))\n",
    "    plt.xlabel('Hyperparameter optimisation progression / n calls')\n",
    "    plt.ylabel('Mean absolute error / kcal mol$^{-1}$')\n",
    "    plt.show()\n",
    "        \n",
    "#     # Setup figure.\n",
    "#     fig, axes = plt.subplots(nrows=1, ncols=len(feature_sets), sharey=True)\n",
    "#     fig.set_size_inches(18.5, 7)\n",
    "    \n",
    "#     for ax, MAE_outputs in zip(axes, MAE_output_dfs):\n",
    "#         MAEs_all_reps = []\n",
    "#         for MAE_output in MAE_outputs:\n",
    "\n",
    "#             result_maes = MAE_output[2]\n",
    "#             name = MAE_output[1]\n",
    "#             # y values\n",
    "#             mae = [result_maes.loc[result_maes.iloc[:, 0] == fold, 'MAE (kcal/mol)'].cummin()\n",
    "#                    for fold in range(1, n_splits + 1)]\n",
    "#             cumm_mae = list(zip(*mae))\n",
    "#             mean_MAEs_this_rep = [statistics.mean(call) for call in cumm_mae]\n",
    "#             MAEs_all_reps.append(mean_MAEs_this_rep)\n",
    "        \n",
    "#         y = np.mean(MAEs_all_reps, axis=0)\n",
    "#         std = np.std(MAEs_all_reps, axis=0)\n",
    "        \n",
    "\n",
    "#         # standard devation bounds\n",
    "#         y1 = [i - sd for i, sd in zip(y, std)]\n",
    "#         y2 = [i + sd for i, sd in zip(y, std)]\n",
    "        \n",
    "#         ax.plot(x, y,\n",
    "#                 linewidth=linewidth,\n",
    "#                 #label='Average MAE over {} folds'.format(n_splits)\n",
    "#                )\n",
    "#         # plot standard deviation fill bounds\n",
    "#         ax.fill_between(x, y1, y2,\n",
    "#                         alpha=0.2,\n",
    "#                         #label='Standard deviation'\n",
    "#                        )\n",
    "#         # more formatting:\n",
    "#         ax.set_title(name)\n",
    "   \n",
    "\n",
    "#     axes[3].set_xlabel('Hyperparameter optimisation progression / n calls', fontsize=fontsize)\n",
    "#     axes[0].set_ylabel('Mean absolute error / kcal mol$^{-1}$', fontsize=fontsize)\n",
    "\n",
    "#     #fig.savefig(figures_dr + 'convergence_plot.png', dpi=dpi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make a list with all our MAE outputs:\n",
    "MAE_output_dfs = []\n",
    "replicate_tags = [ \"repl\"+str(value) for value in np.arange(1,11) ]\n",
    "maes_per_featureset = []\n",
    "for feature_set in feature_sets:\n",
    "    maes_all_replicates = []\n",
    "    for replicate in replicate_tags:\n",
    "        mae_logger = output_dr + feature_set + '/mae_per_call'+replicate+'.csv'\n",
    "        mae_df = pd.read_csv(mae_logger).head(n_calls*n_splits)  # Load previously defined training statistics logger.\n",
    "        maes_all_replicates.append([replicate, feature_set, mae_df])\n",
    "    maes_per_featureset.append(maes_all_replicates)\n",
    "        \n",
    "\n",
    "# Plot convergence plots.\n",
    "\n",
    "mae_convergence(maes_per_featureset)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All generated models and logs are saved in ./output/\n",
    "Note that because of hyperparameter optimision per fold, each fold model will have a different\n",
    "hyperparameter configuration. The specific configurations can be retrieved from the outputs above.\n",
    "\n",
    "\n",
    "The convergence plot is saved ./figures/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
